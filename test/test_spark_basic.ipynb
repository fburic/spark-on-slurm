{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spark test: read and filter tiny CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> NOTE </span>\n",
    "\n",
    "Please set the `context_already_defined` flag depending on your context.\n",
    "\n",
    "If this notebook is started against a pyspark session, there will already be\n",
    "a defined context called `spark`. Attempts to create a new one will fail.\n",
    "\n",
    "If this notebook is meant to start a Spark context in local mode,\n",
    "set `context_already_defined = False`, sine one needs to create a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_already_defined = True\n",
    "\n",
    "if context_already_defined:\n",
    "    sc = spark.sparkContext\n",
    "    sqlc = SQLContext(sc)\n",
    "    \n",
    "else:\n",
    "    from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "    conf = SparkConf().setAppName(\"msmap-filter\").setMaster(\"local[*]\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tiny CSV file and read it into a pyspak Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B\r\n",
      "1,4.45\r\n",
      "2,4.55\r\n",
      "3,7.7\r\n",
      "4,8.2\r\n"
     ]
    }
   ],
   "source": [
    "tiny_csv=\"\"\"\n",
    "A,B\n",
    "1,4.45\n",
    "2,4.55\n",
    "3,7.7\n",
    "4,8.2\"\"\"\n",
    "\n",
    "with open('test_data.csv', 'w') as tiny_csv_file:\n",
    "    tiny_csv_file.write(tiny_csv)\n",
    "    \n",
    "! cat test_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'test_data.csv'\n",
    "\n",
    "ms_schema = StructType([StructField(\"A\", ByteType()),\n",
    "                        StructField(\"B\", FloatType())])\n",
    "\n",
    "test_data = sqlc.read.csv(test_file, schema=ms_schema, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  A|   B|\n",
      "+---+----+\n",
      "|  1|4.45|\n",
      "|  2|4.55|\n",
      "+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please inspect the application SparkUI for stage execution trace and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Details\n",
    "\n",
    "```\n",
    "org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n",
    "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "java.lang.reflect.Method.invoke(Method.java:498)\n",
    "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "py4j.Gateway.invoke(Gateway.java:280)\n",
    "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
    "java.lang.Thread.run(Thread.java:745)\n",
    "```\n",
    "\n",
    "\n",
    "## SQL for stage\n",
    "\n",
    "WholeStageCodegen: Scan csv number of output rows: 4number of files: 1metadata time (ms): 6\n",
    "\n",
    "```\n",
    "== Parsed Logical Plan ==\n",
    "GlobalLimit 3\n",
    "+- LocalLimit 3\n",
    "   +- Relation[A#0,B#1] csv\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "A: tinyint, B: float\n",
    "GlobalLimit 3\n",
    "+- LocalLimit 3\n",
    "   +- Relation[A#0,B#1] csv\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "GlobalLimit 3\n",
    "+- LocalLimit 3\n",
    "   +- Relation[A#0,B#1] csv\n",
    "\n",
    "== Physical Plan ==\n",
    "CollectLimit 3\n",
    "+- *FileScan csv [A#0,B#1] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:<edited>/test_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<A:tinyint,B:float>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple numerical filter on the inner RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=2, B=4.550000190734863), Row(A=4, B=8.199999809265137)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.rdd.filter(lambda row: row['A'] % 2 == 0)\n",
    "test_data.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Details\n",
    "\n",
    "```\n",
    "org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
    "org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n",
    "org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
    "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "java.lang.reflect.Method.invoke(Method.java:498)\n",
    "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "py4j.Gateway.invoke(Gateway.java:280)\n",
    "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
    "java.lang.Thread.run(Thread.java:745)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nteract": {
   "version": "0.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
